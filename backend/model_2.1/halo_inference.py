# -*- coding: utf-8 -*-
"""Halo Inference

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x9WwLNIPbDEQ13rcfkgpm4SrvHg6UndT
"""

import time
import numpy as np
import torch
import torch.nn as nn
import scipy.signal
import joblib

# Optional imports for BrainAccess SDK (reference implementation only)
# This file is a reference/template - the actual implementation uses LSL in brainaccess_live.py
# NOTE: This file is NOT used in production - it's just a reference showing the original model usage
try:
    import brainaccess  # type: ignore
    from brainaccess.utils import acquisition  # type: ignore
    BRAINACCESS_AVAILABLE = True
except ImportError:
    BRAINACCESS_AVAILABLE = False
    # These imports are optional - this is a reference file only
    brainaccess = None  # type: ignore
    acquisition = None  # type: ignore

# --- CONFIGURATION ---
TARGET_CHANNELS = ['AF3', 'AF4', 'O1', 'O2']
TARGET_FS = 250  # The model expects 250Hz
WINDOW_SIZE = 250  # 1 second of data
OVERLAP = 25     # Predict every ~0.1 seconds (sliding window)

# --- 1. MODEL DEFINITION (Must match training exactly) ---
class EmotionClassifier(nn.Module):
    def __init__(self, input_size, num_classes):
        super(EmotionClassifier, self).__init__()
        self.layer1 = nn.Linear(input_size, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.dropout1 = nn.Dropout(0.4)

        self.layer2 = nn.Linear(128, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.dropout2 = nn.Dropout(0.4)

        self.layer3 = nn.Linear(64, 32)

        self.output = nn.Linear(32, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.bn1(self.layer1(x)))
        x = self.dropout1(x)
        x = self.relu(self.bn2(self.layer2(x)))
        x = self.dropout2(x)
        x = self.relu(self.layer3(x))
        x = self.output(x)
        return x

# --- 2. PREDICTOR LOGIC ---
class EmotionPredictor:
    def __init__(self, model_path='eeg_emotion_model.pth', scaler_path='eeg_scaler.pkl'):
        self.device = torch.device('cpu')
        self.bands = {
            'Delta': (0.5, 4), 'Theta': (4, 8), 'Alpha': (8, 13),
            'Beta': (13, 30), 'Gamma': (30, 45)
        }

        print("[System] Loading Model and Scaler...")
        try:
            self.scaler = joblib.load(scaler_path)
            self.model = EmotionClassifier(input_size=20, num_classes=4)
            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
            self.model.to(self.device)
            self.model.eval()
            print("[System] Models loaded successfully.")
        except FileNotFoundError:
            raise Exception("Model files not found! Run the training script first.")

    def get_band_power(self, data, fs):
        eps = 1e-10
        # Calculate PSD
        freqs, psd = scipy.signal.welch(data, fs, nperseg=len(data), axis=0)
        total_power = np.sum(psd, axis=0)
        features = []

        for ch_idx in range(data.shape[1]):
            for band, (low, high) in self.bands.items():
                idx = np.logical_and(freqs >= low, freqs <= high)
                if total_power[ch_idx] == 0:
                    val = 0
                else:
                    val = np.sum(psd[idx, ch_idx]) / (total_power[ch_idx] + eps)
                features.append(val)
        return np.array(features, dtype=np.float32)

    def predict(self, buffer_data):
        # buffer_data shape: (250, 4)
        features = self.get_band_power(buffer_data, fs=TARGET_FS)
        features = features.reshape(1, -1)
        features_scaled = self.scaler.transform(features)

        input_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(self.device)

        with torch.no_grad():
            outputs = self.model(input_tensor)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            _, predicted_idx = torch.max(outputs, 1)

        labels = ['Calm', 'Relaxed', 'Negative/Angry', 'Positive/Happy']
        return labels[predicted_idx.item()], float(probs[0][predicted_idx.item()])

# --- 3. HARDWARE CONNECTION & LOOP ---
def run_live_session():
    if not BRAINACCESS_AVAILABLE:
        print("[Error] brainaccess SDK not available. This is a reference implementation.")
        print("[Info] The actual production code uses LSL streaming in brainaccess_live.py")
        return
    
    # Initialize Predictor
    predictor = EmotionPredictor()

    # 1. Connect to BrainAccess HALO
    print("[Halo] Connecting to device... (Please ensure it is turned on)")
    # This usually opens a connection manager or connects to the first available device
    acquisition.connect()

    # 2. Setup Streaming
    with acquisition.Streaming() as stream:
        print("[Halo] Stream started.")

        # Determine sampling rate and channel mapping
        device_fs = stream.get_sampling_rate()
        print(f"[Halo] Device Sampling Rate: {device_fs} Hz")

        all_channels = stream.get_channel_names()
        print(f"[Halo] Available Channels: {all_channels}")

        # Map target channels to device indices
        channel_indices = []
        for target in TARGET_CHANNELS:
            try:
                idx = all_channels.index(target)
                channel_indices.append(idx)
            except ValueError:
                print(f"[Error] Channel {target} not found on device! Check electrode setup.")
                return

        print(f"[Halo] Monitoring channels: {[all_channels[i] for i in channel_indices]}")

        # Buffer to hold data
        # We need a continuous buffer. We'll append new data and slice the end.
        eeg_buffer = np.zeros((0, len(channel_indices)))

        print("\n--- STARTING LIVE CLASSIFICATION (Ctrl+C to stop) ---\n")

        try:
            while True:
                # A. Get latest data chunk
                # chunk is usually shape (n_samples, n_total_channels)
                chunk = stream.get_data()

                if chunk is None or len(chunk) == 0:
                    time.sleep(0.01)
                    continue

                # B. Transpose if necessary (SDKs often return channels x samples, we need samples x channels)
                # BrainAccess typically returns (n_samples, n_channels) or vice-versa depending on version.
                # We assume standard (n_samples, n_channels). If shape[0] is small (channels), transpose.
                if chunk.shape[0] < chunk.shape[1] and chunk.shape[0] == len(all_channels):
                     chunk = chunk.T

                # C. Extract specific channels
                chunk = chunk[:, channel_indices]

                # D. Resample if hardware is not 250Hz (e.g. if it's 128Hz or 500Hz)
                if device_fs != TARGET_FS:
                    num_samples_target = int(len(chunk) * TARGET_FS / device_fs)
                    if num_samples_target > 0:
                        chunk = scipy.signal.resample(chunk, num_samples_target)

                # E. Update Buffer
                eeg_buffer = np.vstack([eeg_buffer, chunk])

                # F. Check if we have enough data to predict (1 second window)
                if len(eeg_buffer) >= WINDOW_SIZE:
                    # Extract the last 1 second of data
                    window_data = eeg_buffer[-WINDOW_SIZE:]

                    # Run Prediction
                    emotion, conf = predictor.predict(window_data)

                    # Visualization (overwrite line)
                    print(f"\rDetected: \033[92m{emotion.upper()}\033[0m (Conf: {conf:.2f}) | Buffer: {len(eeg_buffer)}", end="")

                    # Keep buffer size manageable (sliding window)
                    # We keep the overlap for the next prediction
                    keep_size = WINDOW_SIZE - OVERLAP
                    eeg_buffer = eeg_buffer[-keep_size:]

                time.sleep(0.01) # Prevent CPU hogging

        except KeyboardInterrupt:
            print("\n[System] Stopping stream...")

if __name__ == "__main__":
    run_live_session()