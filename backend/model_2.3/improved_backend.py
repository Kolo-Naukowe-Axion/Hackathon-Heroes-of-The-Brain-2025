# -*- coding: utf-8 -*-
"""Improved Backend

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZQsrMGp3Y48Blp9BSdchzbAMPgE-CWx7
"""

import time
import numpy as np
import torch
import torch.nn as nn
import scipy.signal
import joblib
import brainaccess
from brainaccess.utils import acquisition

TARGET_CHANNELS = ['AF3', 'AF4', 'O1', 'O2']
TARGET_FS = 250
WINDOW_SIZE = 250

# --- GRU MODEL DEFINITION (Must Match Training Script Exactly) ---
class EEG_GRU(nn.Module):
    def __init__(self, num_classes=4):
        super(EEG_GRU, self).__init__()

        # 1. Convolutional Block
        self.conv_block = nn.Sequential(
            nn.Conv1d(4, 16, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm1d(16),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.2),

            nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm1d(32),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.2)
        )

        # 2. Recurrent Block
        self.gru = nn.GRU(input_size=32, hidden_size=64, num_layers=2, batch_first=True, dropout=0.3)

        # 3. Classifier
        self.fc = nn.Sequential(
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.LeakyReLU(0.1),
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        # x: (Batch, 4, 250)
        x = self.conv_block(x)
        x = x.permute(0, 2, 1) # Swap for GRU: (Batch, Time, Feats)
        out, _ = self.gru(x)
        out = out[:, -1, :]    # Last time step
        out = self.fc(out)
        return out

class GRUPredictor:
    def __init__(self):
        self.device = torch.device('cpu')
        print("Loading GRU Model...")
        try:
            # Load the files generated by train_gru_fast.py
            self.scaler = joblib.load('gru_scaler.pkl')
            self.model = EEG_GRU(num_classes=4)
            self.model.load_state_dict(torch.load('gru_model.pth', map_location=self.device))
            self.model.eval()

            # Filter (0.5 - 45 Hz)
            self.b, self.a = scipy.signal.butter(4, [0.5, 45], btype='band', fs=TARGET_FS)
            print("Model Loaded Successfully.")
        except FileNotFoundError:
            print("Error: Missing gru_model.pth or gru_scaler.pkl. Run training first!")

    def predict(self, raw_buffer):
        """
        raw_buffer: (250, 4) - Time x Channels
        """
        # 1. Transpose to (4, 250)
        data = raw_buffer.T

        # 2. Filter
        data = scipy.signal.filtfilt(self.b, self.a, data, axis=1)

        # 3. Scale
        flat_data = data.reshape(1, -1)
        scaled_data = self.scaler.transform(flat_data)

        # 4. Reshape for GRU: (1, 4, 250)
        input_tensor = scaled_data.reshape(1, 4, 250)
        input_tensor = torch.tensor(input_tensor, dtype=torch.float32)

        # 5. Predict
        with torch.no_grad():
            out = self.model(input_tensor)
            probs = torch.softmax(out, dim=1).numpy()[0]

        classes = ['Calm', 'Relaxed', 'Angry', 'Happy']
        return classes[np.argmax(probs)], probs

def run_live_gru():
    predictor = GRUPredictor()
    acquisition.connect()

    with acquisition.Streaming() as stream:
        all_chans = stream.get_channel_names()
        ids = [all_chans.index(ch) for ch in TARGET_CHANNELS]
        buffer = np.zeros((0, 4))

        print("\n--- LIVE GRU INFERENCE ---")
        while True:
            chunk = stream.get_data()
            if chunk is None or len(chunk) == 0: continue

            if chunk.shape[0] == len(all_chans): chunk = chunk.T
            chunk = chunk[:, ids]

            # Unit check
            if np.mean(np.abs(chunk)) < 0.001: chunk = chunk * 1e6

            buffer = np.vstack([buffer, chunk])

            # Slide logic
            if len(buffer) >= WINDOW_SIZE + 20:
                window = buffer[-WINDOW_SIZE:]

                label, probs = predictor.predict(window)

                # Visual
                p_str = " ".join([f"{p:.2f}" for p in probs])
                print(f"\rPred: {label} [{p_str}]", end="")

                buffer = buffer[-WINDOW_SIZE:]

            time.sleep(0.01)

if __name__ == "__main__":
    run_live_gru()