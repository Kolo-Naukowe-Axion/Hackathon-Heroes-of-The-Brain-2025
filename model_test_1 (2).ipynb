{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "QEEtmvG_gyTm"
      },
      "id": "QEEtmvG_gyTm"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# --- 1. Get Path ---\n",
        "print(\"Locating dataset...\")\n",
        "# If this lines fails or hangs, you can hardcode the path you found earlier:\n",
        "# path_to_dataset = \"/kaggle/input/database-for-emotion-recognition-system-gameemo\"\n",
        "path_to_dataset = kagglehub.dataset_download(\"sigfest/database-for-emotion-recognition-system-gameemo\")\n",
        "print(f\"Dataset Root: {path_to_dataset}\")\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "# BrainAccess HALO Channels\n",
        "TARGET_CHANNELS = ['AF3', 'AF4', 'O1', 'O2']\n",
        "BANDS = {'Delta': (0.5, 4), 'Theta': (4, 8), 'Alpha': (8, 13), 'Beta': (13, 30), 'Gamma': (30, 45)}\n",
        "\n",
        "def get_band_power(data, fs=128):\n",
        "    freqs, psd = scipy.signal.welch(data, fs, nperseg=len(data), axis=0)\n",
        "    total_power = np.sum(psd, axis=0)\n",
        "    features = []\n",
        "    for ch_idx in range(data.shape[1]):\n",
        "        for band, (low, high) in BANDS.items():\n",
        "            idx = np.logical_and(freqs >= low, freqs <= high)\n",
        "            val = 0 if total_power[ch_idx] == 0 else np.sum(psd[idx, ch_idx]) / total_power[ch_idx]\n",
        "            features.append(val)\n",
        "    return np.array(features)\n",
        "\n",
        "def load_data_robust(root_path):\n",
        "    X, y = [], []\n",
        "    count = 0\n",
        "\n",
        "    print(f\"Starting crawl through: {root_path}\")\n",
        "\n",
        "    # Walk through the directory tree\n",
        "    for root, dirs, files in os.walk(root_path):\n",
        "        for filename in files:\n",
        "            # 1. Filter: specific CSVs only\n",
        "            if not filename.endswith(\".csv\"): continue\n",
        "            if \"AllRawChannels\" not in filename: continue\n",
        "\n",
        "            # 2. Determine 4-Class Label\n",
        "            # G1=LANV(0), G2=LAPV(1), G3=HANV(2), G4=HAPV(3)\n",
        "            if \"G1\" in filename:   label = 0\n",
        "            elif \"G2\" in filename: label = 1\n",
        "            elif \"G3\" in filename: label = 2\n",
        "            elif \"G4\" in filename: label = 3\n",
        "            else: continue # Skip if no G code found\n",
        "\n",
        "            file_path = os.path.join(root, filename)\n",
        "\n",
        "            try:\n",
        "                # 3. Load & Process\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                # Find columns (handle spaces like \" AF3\")\n",
        "                cols = [c for c in df.columns if any(t in c for t in TARGET_CHANNELS)]\n",
        "\n",
        "                if len(cols) < 4:\n",
        "                    # Optional: Print skipped files to debug\n",
        "                    # print(f\"Skipping {filename}: Found {len(cols)}/4 channels\")\n",
        "                    continue\n",
        "\n",
        "                raw_data = df[cols].values\n",
        "                fs = 128\n",
        "\n",
        "                # Segment (1 sec)\n",
        "                for i in range(len(raw_data) // fs):\n",
        "                    window = raw_data[i*fs : (i+1)*fs]\n",
        "                    X.append(get_band_power(window, fs))\n",
        "                    y.append(label)\n",
        "\n",
        "                count += 1\n",
        "                if count % 10 == 0: print(f\"Processed {count} files...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error on {filename}: {e}\")\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# --- EXECUTE ---\n",
        "X, y = load_data_robust(path_to_dataset)\n",
        "\n",
        "print(\"\\nProcessing Complete.\")\n",
        "print(f\"Features: {X.shape}\")\n",
        "print(f\"Labels: {y.shape}\")\n",
        "print(f\"Classes: {np.unique(y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViVZLgOWgxeQ",
        "outputId": "d565dbe1-be68-411c-d68f-fa2e67af6ef4"
      },
      "id": "ViVZLgOWgxeQ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Locating dataset...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/sigfest/database-for-emotion-recognition-system-gameemo?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.70G/1.70G [00:13<00:00, 139MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Root: /root/.cache/kagglehub/datasets/sigfest/database-for-emotion-recognition-system-gameemo/versions/1\n",
            "Starting crawl through: /root/.cache/kagglehub/datasets/sigfest/database-for-emotion-recognition-system-gameemo/versions/1\n",
            "Processed 10 files...\n",
            "Processed 20 files...\n",
            "Processed 30 files...\n",
            "Processed 40 files...\n",
            "Processed 50 files...\n",
            "Processed 60 files...\n",
            "Processed 70 files...\n",
            "Processed 80 files...\n",
            "Processed 90 files...\n",
            "Processed 100 files...\n",
            "\n",
            "Processing Complete.\n",
            "Features: (32184, 20)\n",
            "Labels: (32184,)\n",
            "Classes: [0 1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import copy"
      ],
      "metadata": {
        "id": "nT_QCOtfg2PV"
      },
      "id": "nT_QCOtfg2PV",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'X' not in locals() or 'y' not in locals():\n",
        "    raise ValueError(\"Please run the data loading cell first.\")\n",
        "\n",
        "print(\"Step 1: Preparing Hybrid Features...\")\n",
        "# Log transform to normalize EEG power distribution\n",
        "X_log = np.log1p(X)\n",
        "\n",
        "# Polynomial Features (Interaction terms)\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_log)\n",
        "\n",
        "# Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_poly)\n",
        "\n",
        "# Split Data\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.15, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Convert to Tensors for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "X_train_torch = torch.tensor(X_train_raw, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_torch = torch.tensor(X_test_raw, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
        "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GA1mnhzh9VU",
        "outputId": "e3d5b284-30a0-4f23-af09-3bb0d654b0fe"
      },
      "id": "6GA1mnhzh9VU",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Preparing Hybrid Features...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, dropout_rate):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "        )\n",
        "        self.activation = nn.GELU()\n",
        "    def forward(self, x):\n",
        "        return self.activation(x + self.block(x))\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=4):\n",
        "        super(WideResNet, self).__init__()\n",
        "        hidden_dim = 1024\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            ResidualBlock(hidden_dim, 0.4),\n",
        "            ResidualBlock(hidden_dim, 0.4),\n",
        "            ResidualBlock(hidden_dim, 0.4)\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.res_blocks(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "s7L9d1x7iBiJ"
      },
      "id": "s7L9d1x7iBiJ",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 2: Training WideResNet...\")\n",
        "resnet_model = WideResNet(input_dim=X_scaled.shape[1]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.AdamW(resnet_model.parameters(), lr=0.001, weight_decay=1e-3)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, steps_per_epoch=len(train_loader), epochs=100)\n",
        "\n",
        "best_acc = 0.0\n",
        "best_weights = copy.deepcopy(resnet_model.state_dict())\n",
        "\n",
        "for epoch in range(100):\n",
        "    resnet_model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Quick Validation Check\n",
        "    resnet_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = resnet_model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_weights = copy.deepcopy(resnet_model.state_dict())\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}/100 - ResNet Val Acc: {acc:.2f}%\")\n",
        "\n",
        "# Load best weights\n",
        "resnet_model.load_state_dict(best_weights)\n",
        "print(f\"Best ResNet Accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "# --- 3. Train Model B: Gradient Boosting (Tree-based) ---\n",
        "print(\"\\nStep 3: Training Gradient Boosting Classifier...\")\n",
        "# HistGradientBoosting is typically faster and more accurate for this scale than standard RF\n",
        "gb_model = HistGradientBoostingClassifier(\n",
        "    learning_rate=0.1,\n",
        "    max_iter=200,\n",
        "    max_leaf_nodes=31,\n",
        "    l2_regularization=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train_raw, y_train)\n",
        "gb_acc = gb_model.score(X_test_raw, y_test) * 100\n",
        "print(f\"Gradient Boosting Accuracy: {gb_acc:.2f}%\")\n",
        "\n",
        "# --- 4. Ensemble Prediction (Voting) ---\n",
        "print(\"\\nStep 4: Combining Models (Ensemble)...\")\n",
        "\n",
        "# Get Probabilities from ResNet\n",
        "resnet_model.eval()\n",
        "all_resnet_probs = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        logits = resnet_model(inputs)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        all_resnet_probs.append(probs)\n",
        "resnet_probs = np.concatenate(all_resnet_probs)\n",
        "\n",
        "# Get Probabilities from Gradient Boosting\n",
        "gb_probs = gb_model.predict_proba(X_test_raw)\n",
        "\n",
        "# Weighted Average (Give slightly more weight to the better model usually, but 50/50 is robust)\n",
        "# If ResNet is ~77% and GB is ~75%, a 0.6/0.4 split often works well.\n",
        "final_probs = (0.6 * resnet_probs) + (0.4 * gb_probs)\n",
        "final_preds = np.argmax(final_probs, axis=1)\n",
        "\n",
        "# Final Evaluation\n",
        "ensemble_acc = accuracy_score(y_test, final_preds) * 100\n",
        "print(f\"\\n>>> FINAL ENSEMBLE ACCURACY: {ensemble_acc:.2f}% <<<\")\n",
        "\n",
        "class_names = ['LANV (Boring)', 'LAPV (Calm)', 'HANV (Horror)', 'HAPV (Funny)']\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, final_preds, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB7tkylviF29",
        "outputId": "4506b1c0-ea0f-4318-c021-06ab32aae8e9"
      },
      "id": "eB7tkylviF29",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Training WideResNet...\n",
            "Epoch 20/100 - ResNet Val Acc: 51.14%\n",
            "Epoch 40/100 - ResNet Val Acc: 66.03%\n",
            "Epoch 60/100 - ResNet Val Acc: 78.67%\n",
            "Epoch 80/100 - ResNet Val Acc: 82.66%\n",
            "Epoch 100/100 - ResNet Val Acc: 83.20%\n",
            "Best ResNet Accuracy: 83.49%\n",
            "\n",
            "Step 3: Training Gradient Boosting Classifier...\n",
            "Gradient Boosting Accuracy: 62.72%\n",
            "\n",
            "Step 4: Combining Models (Ensemble)...\n",
            "\n",
            ">>> FINAL ENSEMBLE ACCURACY: 83.53% <<<\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "LANV (Boring)       0.82      0.87      0.85      1207\n",
            "  LAPV (Calm)       0.83      0.81      0.82      1207\n",
            "HANV (Horror)       0.83      0.84      0.84      1207\n",
            " HAPV (Funny)       0.85      0.82      0.84      1207\n",
            "\n",
            "     accuracy                           0.84      4828\n",
            "    macro avg       0.84      0.84      0.84      4828\n",
            " weighted avg       0.84      0.84      0.84      4828\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v1KTZFFortEc"
      },
      "id": "v1KTZFFortEc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}